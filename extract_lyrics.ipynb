{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "runtime_attributes": {
        "runtime_version": "2025.07"
      }
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Before using the code, first click on \"Runtime\" → \"Change runtime type\",\n",
        "then select \"T4-GPU\" for the \"Hardware accelerator\" option, and choose \"2025.07\" for the \"Runtime version\"\n",
        " (to adapt to the Spleeter version, as the latest Runtime with default Python 3.12 version is incompatible with Spleeter)\n",
        "'''\n",
        "!apt-get install -y ffmpeg\n",
        "!pip install spleeter==2.4.2\n"
      ],
      "metadata": {
        "id": "nVgNcsfaY7XD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install --upgrade --no-deps --force-reinstall git+https://github.com/openai/whisper.git\n",
        "\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install pandas tqdm"
      ],
      "metadata": {
        "id": "WX8BKjKjUyQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "sCHfk1IlrlyY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f7dc029-da0b-4985-b7b3-5dbad47fbe17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import torch\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "\n",
        "\n",
        "AUDIO_DIR = \"/content/drive/MyDrive/MEMD_audio\"  # Audio file storage path\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/separated_vocals\"  # The output path of the separated voices\n",
        "RESULTS_DIR = \"/content/drive/MyDrive/results\"  # Result path\n",
        "BATCH_SIZE = 50\n",
        "\n",
        "\n",
        "!mkdir -p {OUTPUT_DIR}\n",
        "!mkdir -p {RESULTS_DIR}\n",
        "\n",
        "\n",
        "def get_audio_files(directory, extensions=['.mp3', '.wav']):\n",
        "    files = []\n",
        "    for ext in extensions:\n",
        "        files.extend(glob.glob(os.path.join(directory, f\"*{ext}\")))\n",
        "    return files\n",
        "\n",
        "\n",
        "def batch_process(file_list, batch_size=100):\n",
        "    for i in range(0, len(file_list), batch_size):\n",
        "        yield file_list[i:i + batch_size]\n",
        "\n",
        "\n",
        "def separate_vocals_batch(audio_files, output_dir):\n",
        "    print(f\"processing {len(audio_files)} audio files...\")\n",
        "\n",
        "\n",
        "    for i, audio_file in enumerate(tqdm(audio_files)):\n",
        "        file_name = os.path.basename(audio_file)\n",
        "        song_id = os.path.splitext(file_name)[0]\n",
        "        song_output_dir = os.path.join(output_dir, song_id)\n",
        "\n",
        "        print(f\"处理 {i+1}/{len(audio_files)}: {file_name}\")\n",
        "\n",
        "        !spleeter separate -p spleeter:2stems -o {output_dir} \"{audio_file}\"\n",
        "\n",
        "        if (i+1) % 20 == 0:\n",
        "            print(f\"Finished: {i+1}/{len(audio_files)} files\")\n",
        "\n",
        "def main_separation():\n",
        "    all_audio_files = get_audio_files(AUDIO_DIR)\n",
        "    print(f\"found {len(all_audio_files)} audio files in total\")\n",
        "\n",
        "    with open(f\"{RESULTS_DIR}/process_parameters.txt\", 'w') as f:\n",
        "        f.write(f\"processing time: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "        f.write(f\"audio dictionary: {AUDIO_DIR}\\n\")\n",
        "        f.write(f\"output dictionary: {OUTPUT_DIR}\\n\")\n",
        "        f.write(f\"result dictionary: {RESULTS_DIR}\\n\")\n",
        "        f.write(f\"audio files count: {len(all_audio_files)}\\n\")\n",
        "        f.write(f\"device: {device}\\n\")\n",
        "\n",
        "    for batch_idx, batch_files in enumerate(batch_process(all_audio_files, BATCH_SIZE)):\n",
        "        print(f\"processing batch {batch_idx+1}: {len(batch_files)} files\")\n",
        "        print(\"=== Start voice separation ===\")\n",
        "        separate_vocals_batch(batch_files, OUTPUT_DIR)\n",
        "\n",
        "        print(f\"batch {batch_idx+1} finished!\")\n",
        "\n",
        "    print(\"All voice separation has been completed！\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_separation()\n",
        "else:\n",
        "    main_separation()"
      ],
      "metadata": {
        "id": "lzy8tRE6jpyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3GHFaIGqS5j"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import torch\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/separated_vocals\"  # The output path of the separated voices\n",
        "RESULTS_DIR = \"/content/drive/MyDrive/results\"  # Result path\n",
        "BATCH_SIZE = 50\n",
        "WHISPER_MODEL = \"medium\"  # options：tiny, base, small, medium, large\n",
        "\n",
        "!mkdir -p {RESULTS_DIR}\n",
        "\n",
        "def batch_process(file_list, batch_size=50):\n",
        "    for i in range(0, len(file_list), batch_size):\n",
        "        yield file_list[i:i + batch_size]\n",
        "\n",
        "def transcribe_vocals_batch(vocals_dir, whisper_model=\"small\", language=\"en\"):\n",
        "    import whisper\n",
        "    print(f\"Loading Whisper model: {whisper_model}\")\n",
        "    model = whisper.load_model(whisper_model, device=device)\n",
        "    vocals_files = glob.glob(os.path.join(vocals_dir, \"*/vocals.wav\"))\n",
        "    print(f\"found {len(vocals_files)} voice files\")\n",
        "    results = []\n",
        "\n",
        "    for i, vocal_file in enumerate(tqdm(vocals_files)):\n",
        "        song_id = os.path.basename(os.path.dirname(vocal_file))\n",
        "        try:\n",
        "            print(f\"transcribing {i+1}/{len(vocals_files)}: {song_id}\")\n",
        "            start_time = time.time()\n",
        "            result = model.transcribe(vocal_file, language=language)\n",
        "            processing_time = time.time() - start_time\n",
        "            results.append({\n",
        "                \"song_id\": song_id,\n",
        "                \"lyrics\": result[\"text\"],\n",
        "                \"processing_time\": processing_time,\n",
        "                \"segments\": result[\"segments\"]\n",
        "            })\n",
        "            print(f\"Time-consuming: {processing_time:.2f}s\")\n",
        "            print(f\"result: {result['text'][:100]}...\" if len(result[\"text\"]) > 100 else result[\"text\"])\n",
        "\n",
        "            if torch.cuda.is_available() and (i+1) % 10 == 0:\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"error when processing {song_id} : {str(e)}\")\n",
        "            results.append({\n",
        "                \"song_id\": song_id,\n",
        "                \"error\": str(e)\n",
        "            })\n",
        "\n",
        "        if (i+1) % 50 == 0:\n",
        "            temp_df = pd.DataFrame(results)\n",
        "            temp_df.to_csv(f\"{RESULTS_DIR}/whisper_lyrics_batch_{(i+1)//50}.csv\", index=False)\n",
        "            with open(f\"{RESULTS_DIR}/whisper_details_batch_{(i+1)//50}.json\", 'w', encoding='utf-8') as f:\n",
        "                import json\n",
        "                json.dump(results, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    final_df = pd.DataFrame([{\n",
        "        \"song_id\": r[\"song_id\"],\n",
        "        \"lyrics\": r.get(\"lyrics\", \"\"),\n",
        "        \"error\": r.get(\"error\", \"\"),\n",
        "        \"processing_time\": r.get(\"processing_time\", 0)\n",
        "    } for r in results])\n",
        "\n",
        "    final_df.to_csv(f\"{RESULTS_DIR}/whisper_lyrics_all.csv\", index=False)\n",
        "    with open(f\"{RESULTS_DIR}/whisper_details_all.json\", 'w', encoding='utf-8') as f:\n",
        "        import json\n",
        "        json.dump(results, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    return results\n",
        "\n",
        "def main_transcription():\n",
        "    with open(f\"{RESULTS_DIR}/transcription_parameters.txt\", 'w') as f:\n",
        "        f.write(f\"Time-consuming: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "        f.write(f\"output dir: {OUTPUT_DIR}\\n\")\n",
        "        f.write(f\"reslut dir: {RESULTS_DIR}\\n\")\n",
        "        f.write(f\"Whisper model: {WHISPER_MODEL}\\n\")\n",
        "        f.write(f\"device: {device}\\n\")\n",
        "\n",
        "    print(\"=== Start lyrics recognition ===\")\n",
        "    transcribe_results = transcribe_vocals_batch(OUTPUT_DIR, WHISPER_MODEL)\n",
        "    print(\"Lyrics recognition completed, results saved.\")\n",
        "\n",
        "    if transcribe_results:\n",
        "        sample = np.random.choice(transcribe_results)\n",
        "        if \"lyrics\" in sample:\n",
        "            print(\"\\n=== Random result example ===\")\n",
        "            print(f\"song ID: {sample['song_id']}\")\n",
        "            print(f\"lyric content:\\n{sample['lyrics']}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_transcription()\n",
        "else:\n",
        "    main_transcription()"
      ]
    }
  ]
}